{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885fe425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bd45be85f04d8193e0f0f1de1c6a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>6</td><td>application_1675774464703_0007</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketname: String = unibo-bd2223-paolopenazzi\n"
     ]
    }
   ],
   "source": [
    "val bucketname = \"unibo-bd2223-paolopenazzi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bafc0",
   "metadata": {},
   "source": [
    "# DataPreparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b7adc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72c3bf1ef05437cb0ef6bdbfb35fbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 13.0 failed 4 times, most recent failure: Lost task 3.3 in stage 13.0 (TID 25, ip-172-31-82-188.ec2.internal, executor 4): java.lang.NumberFormatException: For input string: \"P1DT1\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
      "\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n",
      "\tat FlightData$.parse(<console>:39)\n",
      "\tat $anonfun$2.apply(<console>:29)\n",
      "\tat $anonfun$2.apply(<console>:29)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1864)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2171)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2159)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2158)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2158)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1011)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1011)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1011)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2419)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2368)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2357)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:822)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2111)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2151)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2176)\n",
      "  at org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n",
      "  ... 50 elided\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"P1DT1\"\n",
      "  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "  at java.lang.Integer.parseInt(Integer.java:580)\n",
      "  at java.lang.Integer.parseInt(Integer.java:615)\n",
      "  at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)\n",
      "  at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n",
      "  at FlightData$.parse(<console>:39)\n",
      "  at $anonfun$2.apply(<console>:29)\n",
      "  at $anonfun$2.apply(<console>:29)\n",
      "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1864)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
      "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class FlightData(\n",
    "    flightID:String,\n",
    "    searchDate:String,\n",
    "    searchMonth:String,\n",
    "    searchDay:String,\n",
    "    flightDate:String,\n",
    "    flightMonth:String,\n",
    "    flightDay:String,\n",
    "    startingAirport:String,\n",
    "    destinationAirport:String,\n",
    "    duration:Int,\n",
    "    isEconomy:Boolean,\n",
    "    isRefundable:Boolean,\n",
    "    isNonStop:Boolean,\n",
    "    baseFare:Double,\n",
    "    totalFare:Double,\n",
    "    seatsRemaining:Int,\n",
    "    travelDistance:Int\n",
    ")\n",
    "\n",
    "object FlightData {\n",
    "    def parse(line:String) = {\n",
    "        val input = line.split(\",\")\n",
    "        val flightID = input(0)\n",
    "        val searchDate = input(1)\n",
    "        val searchMonth = searchDate.substring(5,7)\n",
    "        val searchDay = searchDate.substring(8,10)\n",
    "        val flightDate = input(2)\n",
    "        val flightMonth = flightDate.substring(5,7)\n",
    "        val flightDay = flightDate.substring(8,10)\n",
    "        val startingAirport = input(3)\n",
    "        val destinationAirport = input(4)\n",
    "        val dur = input(6).replace(\"P\",\"\").replace(\"T\",\"\").split(\"D|H|M\")\n",
    "        val duration = durationArray.length match {\n",
    "            case 3 => dur(0) * 1440 + dur(1) * 60 + dur(2)\n",
    "            case 2 => \n",
    "        }\n",
    "        val isEconomy = input(8).toBoolean\n",
    "        val isRefundable = input(9).toBoolean\n",
    "        val isNonStop = input(10).toBoolean\n",
    "        val baseFare = input(11).toDouble\n",
    "        val totalFare = input(12).toDouble\n",
    "        val seatsRemaining = input(13).toInt\n",
    "        val travelDistance = input(14) match {\n",
    "            case \"\" => 0\n",
    "            case _ => input(14).toInt\n",
    "        }\n",
    "        new FlightData(flightID,searchDate,searchMonth,searchDay,flightDate,flightMonth,flightDay,startingAirport,\n",
    "                       destinationAirport,duration,isEconomy,isRefundable,isNonStop,baseFare,totalFare,\n",
    "                       seatsRemaining,travelDistance)\n",
    "    }\n",
    "}\n",
    "\n",
    "val rdd = sc.textFile(\"s3a://\"+bucketname+\"/datasets/itineraries.csv\")\n",
    "val header = rdd.first(); //extract header\n",
    "val rddFlights = rdd.filter(row => row != header).map(FlightData.parse)\n",
    "rddFlights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a910f913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fa42a845dd440f9fbeafd19ca29641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddTest: org.apache.spark.rdd.RDD[String] = s3a://unibo-bd2223-paolopenazzi/datasets/itineraries.csv MapPartitionsRDD[39] at textFile at <console>:27\n",
      "header: String = legId,searchDate,flightDate,startingAirport,destinationAirport,fareBasisCode,travelDuration,elapsedDays,isBasicEconomy,isRefundable,isNonStop,baseFare,totalFare,seatsRemaining,totalTravelDistance,segmentsDepartureTimeEpochSeconds,segmentsDepartureTimeRaw,segmentsArrivalTimeEpochSeconds,segmentsArrivalTimeRaw,segmentsArrivalAirportCode,segmentsDepartureAirportCode,segmentsAirlineName,segmentsAirlineCode,segmentsEquipmentDescription,segmentsDurationInSeconds,segmentsDistance,segmentsCabinCode\n",
      "rddA: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[41] at filter at <console>:27\n",
      "res6: String = 6e643b6e16e47139393c833ec90c686b,2022-05-21,2022-07-04,OAK,DEN,XAVNA0ME,P1DT1H27M,1,False,False,False,244.66,292.21,9,,1656939600||1657026000,2022-07-04T06:00:00.000-07:00||2022-07-05T07:00:00.000-06:00,1656946380||1657031220,2022-07-04T08:53:00.000-06:00||2022-07-05T08:27:00.000-06:00,SLC||DEN,OAK||SLC,Delta||Delta,DL||DL,Airbus A220-100||Boeing 737-800,6780||5220,None||None,coach||coach\n"
     ]
    }
   ],
   "source": [
    "val rddTest = sc.textFile(\"s3a://\"+bucketname+\"/datasets/itineraries.csv\")\n",
    "val header = rddTest.first(); //extract header\n",
    "val rddA = rddTest.filter(row => row != header).filter(x => x.contains(\"P1DT1\"))\n",
    "rddA.take(1000).last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb89aefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a935fadfde4b2eb15fc256171b735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: String = P1DT1H27M\n",
      "array: Array[String] = Array(1, 1, 27)\n"
     ]
    }
   ],
   "source": [
    "val test = \"P1DT1H27M\"\n",
    "val array = test.replace(\"P\",\"\").replace(\"T\",\"\").split(\"D|H|M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "val columns = Seq(\"flightID\",\"searchDate\",\"searchMonth\",\"searchDay\",\"flightDate\",\"flightMonth\",\"flightDay\",\"startingAirport\",\"destinationAirport\",\n",
    "     \"duration\",\"isEconomy\",\"isRefundable\",\"isNonStop\",\"baseFare\",\"totalFare\",\"seatsRemaining\",\"travelDistance\")\n",
    "val flightDataframe = rddFlights.toDF(columns:_*)\n",
    "flightDataframe.show(60,truncate=40,vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.LocalDateTime\n",
    "import java.time.Duration\n",
    "import java.time.temporal.ChronoUnit.DAYS\n",
    "import java.time.LocalDate\n",
    "\n",
    "type rddType = (String,String,String,String,String,String,String,String,String,Int,Boolean,Boolean,Boolean,Double,Double,Int,Int)\n",
    "\n",
    "def enoughData(x: rddType): Long = {\n",
    "        val searchDate = LocalDate.parse(x._2);\n",
    "        val flightDate = LocalDate.parse(x._5);\n",
    "        val daysBetween = DAYS.between(searchDate, flightDate);\n",
    "        daysBetween\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rddFiltered = rddFlights.filter(x => x.searchDate != \"\" && x.flightDate != \"\")\n",
    "rddFiltered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da410ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.LocalDateTime\n",
    "import java.time.Duration\n",
    "import java.time.temporal.ChronoUnit.DAYS\n",
    "import java.time.LocalDate\n",
    "\n",
    "rddFlights.filter(x => DAYS.between(LocalDate.parse(x.searchDate), LocalDate.parse(x.flightDate)) > 7).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val filtered = rddFlightsParsed.filter(x => x._2.equals(\"\"))\n",
    "filtered.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddFlights.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val testA = \"2022-04-16\"\n",
    "val testB = \"2022-05-16\"\n",
    "val dtf = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\");\n",
    "val A = LocalDate.parse(testA, dtf);\n",
    "val B = LocalDate.parse(testB, dtf);\n",
    "val daysBetween = DAYS.between(A, B);\n",
    "daysBetween > 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acccb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
